\chapter{Convolutional Neural Networks}\label{ch:cnn}

% This section provides an overview of the latest in techniques for neural networks. In recent  years it has become apparent that the use of convolution in neural networks is the preferred method of avoiding overfitting when dealing with image data. The goal of this chapter is to act as a reference for the terms and intuitions concerning convolutional neural networks, or to serve as a starting point for further studies. The material in this chapter is not part of the exam, you may consider this chapter (including the following appendices) a large aside in the reader\footnote{For an even further explanation of Convolutional Neural Networks, please consider watching the excellent tutorial by Stanford University: \url{https://youtu.be/AQirPKrAyDg}.}.

\section{Introduction}
Convolutional neural networks (CNNs or ConvNets) are very similar to ordinary neural networks as presented in chapter \ref{ch:nn}: they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs and performs a dot product (see chapter \ref{ch:opencl}). The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. All tips and tricks for learning regular neural networks also still apply.

What makes CNNs different is that they make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function (the classification) more efficient to implement and vastly reduces the amount of parameters in the network (which, in turn, increases the efficiency in learning).

Regular neural networks do not scale well with full images. Small image datasets, like CIFAR-10, which contains images of only $32\times 32\times 3$ ($32$ pixels wide, $32$ pixels high, $3$ colour channels), can still be handled with a regular neural network, but requires a lot of weights. A single fully-connected neuron in a first hidden layer would have $32*32*3=3072$ weights (the weights are often also called \textit{parameters}). While this still seems manageable, consider such a network for a slightly larger image; e.g., $200\times 200\times 3$, which leads to neurons with $200*200*3=120,000$ weights. Not to mention that we would probably want to have multiple of such neurons, which means the number of parameters of the network sky-rockets rather fast.

ConvNets, however, take advantage of the fact that the input consists of images and they constrain the architecture in a more sensible way. In particular, unlike regular neural networks, the layers of a ConvNet have neurons arranged in three dimension: \textbf{width}, \textbf{height}, \textbf{depth}\footnote{Please note that \textit{depth} is typically reserved to denote the number of (hidden) layers of a neural network. Here \textit{depth} denotes the third dimension of the activation volume of a particular layer in the network.}. For example, the input images in CIFAR-10 are an input volume of activations, and the volume has dimensions $32\times 32\times 3$ (width, height, depth, respectively, see right-hand side of Figure \ref{fig:cnnArch}).

\begin{figure}[h!]
\begin{center}
\begin{tikzpicture}[scale=0.8]
\draw[fill=hured!20,hured!20] (0,0) rectangle (1, 3);
\draw[fill=white] (0.5, 0.5) circle (0.4cm);
\draw[fill=white] (0.5, 1.5) circle (0.4cm);
\draw[fill=white] (0.5, 2.5) circle (0.4cm);
\draw[fill=hublue!20,hublue!20] (2,-0.5) rectangle (3,3.5);
\draw[fill=white] (2.5, 0) circle (0.4cm);
\draw[fill=white] (2.5, 1) circle (0.4cm);
\draw[fill=white] (2.5, 2) circle (0.4cm);
\draw[fill=white] (2.5, 3) circle (0.4cm);
\draw[fill=hublue!20,hublue!20] (4.5,-0.5) rectangle (5.5,3.5);
\draw[fill=white] (5, 0) circle (0.4cm);
\draw[fill=white] (5, 1) circle (0.4cm);
\draw[fill=white] (5, 2) circle (0.4cm);
\draw[fill=white] (5, 3) circle (0.4cm);
\draw[fill=green!20,green!20] (6.5,1) rectangle (7.5,2);
\draw[fill=white] (7,1.5) circle (0.4cm);
\foreach \x in {0.5, 1.5, 2.5} \foreach \y in {0,1,2,3} {
  \draw[->] (0.9,\x) to (2.1,\y);
}
\foreach \x in {0,1,2,3} \foreach \y in {0,1,2,3} {
  \draw[->] (2.9,\x) to (4.6,\y);
}
\foreach \x in {0,1,2,3} {
  \draw[->] (5.4,\x) to (6.6,1.5);
}
\node at (0.5, -0.5) {\scriptsize\color{hured!60}{input layer}};
\node at (2.5, -1) {\scriptsize\color{hublue}{hidden layer 1}};
\node at (5, -1) {\scriptsize\color{hublue}{hidden layer 1}};
\node at (7, 0.5) {\scriptsize\color{green}{output layer}};
\end{tikzpicture}~\\[1cm]
\begin{tikzpicture}[scale=0.45]
\draw[fill=hured!20,xyp=7] (0,0) rectangle (1,5);
\draw[fill=hured!40,yzp=1] (0,0) rectangle (5,7);
\draw[fill=hured!40,xzp=5] (0,0) rectangle (1,7);

\draw[fill=hublue!20,xyp=5] (5,0) rectangle (7,3);
\draw[fill=hublue!40,yzp=7] (0,0) rectangle (3,5);
\draw[fill=hublue!40,xzp=3] (5,0) rectangle (7,5);

\draw[fill=hublue!20,xyp=4] (11,0) rectangle (16,3);
\draw[fill=hublue!40,yzp=16] (0,1) rectangle (3,4);
\draw[fill=hublue!40,xzp=3] (11,1) rectangle (16,4);

\draw[decorate,decoration={brace,amplitude=5pt},thick] (16,0,4.2) -- (11.1,0,4.2);
\draw[decorate,decoration={brace,amplitude=5pt},thick] (16.1,0,1) -- (16.1,0,4);
\draw[decorate,decoration={brace,amplitude=5pt},thick] (16.1,3,1) -- (16.1,0.1,1);
\node at (14.5,0,6.5) {\scriptsize depth};
\node[rotate=45] at (17.5,0,4) {\scriptsize width};
\node[rotate=90] at (17,1.5,1) {\scriptsize height};

\draw[fill=green!20,xyp=3] (20,0) rectangle (24,1);
\draw[fill=green!40,yzp=24] (0,2) rectangle (1,3);
\draw[fill=green!40,xzp=1] (20,2) rectangle (24,3);

\draw[->,ultra thick] (1.5,0) to (2.5,0);
\draw[->,ultra thick] (7.5,0) to (8.5,0);
\draw[->,ultra thick] (16.5,0) to (17.5,0);
\end{tikzpicture}
\end{center}
\caption{Top: a regular 3-layer neural network. Bottom: a convnet arranges its neurons in three dimensions (width, height, depth). Each layer in a convnet transforms the 3D input volume to a 3D output volume of neuron activations.}\label{fig:cnnArch}
\end{figure}

\section{General}
As described above, a simple ConvNet is a sequence of layers, where every layer transforms one volume of activations into another through a differentiable function. We distinguish three main types of layers in a convnet architecture: \textit{convolutional layer}, \textit{pooling layer}, and \textit{fully-connected layer} (this latter functions exactly the same as in regular neural networks). A stack of such layers forms a full convent architecture.

A simple example for the CIFAR-10 classification could have the architecture [INPUT -- CONV -- RELU -- POOL -- FC]. In more detail:
\begin{itemize}
\item INPUT [$32\times 32\times 3$] holds the raw pixel values of the image, in this case images with width 32, height 32, and three colour channels (R, G, B);
\item CONV layer computes the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in a volume such as [$32\times 32\times 12$] if we decide to use 12 filters;
\item RELU layer applies an elementwise activation function, such as the $max(0,x)$ threshold at zero. This leaves the size of the volume unchanged;
\item POOL layer performs a downsampling operation along the spatial dimensions (width, height), resulting in a volume such as [$16\times 16\times 12$];
\item FC (i.e., fully-connected) layer computes the class scores, resulting in a volume of size [$1\times 1\times 10$], where each of the 10 numbers corresponds to a class score (the CIFAR-10 dataset had 10 different classes, including cars, airplanes, etc.). As with ordinary neural networks and as the name implies, each neuron on this layer is connected to all the neurons on the previous volume.
\end{itemize}

In this way, convnets transform the original image layer by layer from the original pixel values to the final class scores. Note that some layers contain parameters (CONV and FC) and others do not (INPUT, RELU and POOL). The CONV/FC layers perform transformations that are a function of not only the activations in the input volume, but also of the parameters (the weights and biases of the neurons). On the other hand, the RELU/POOL layers implement a fixed function. The parameters in the CONV/FC layers are trained with gradient descent (see Section \ref{sec:gradient}) such that the class scores that the convnet computes is consistent with the labels in the training set.

\subsection{Convolutional layer}
CONV layers consist of a set of learnable \textit{filters} (sometimes also called \textit{kernels}). Every filter is small spatially (along width and height), but extends through the full depth of the input volume. For example, a typical layer on a first layer of a convnet might have the size $5\times 5\times 3$ (that is, 5 pixels wide and high, and 3 because input images have depth 3). During the forward pass, each filter is slid across the width and height of the input volume and is used to compute the dot products between the entries of the filter and the input at any position. As the filter slides over the width and height of the input volume, a 2-dimensional activation map is produced that gives the responses of that filter at every spatial position. Intuitively, the network learns filters that activate when they see some type of visual feature such as an edge of some orientation or a blotch of some colour on the first layer, or eventually combinations of such patterns, like entire honeycomb or wheel-like patterns, on later layers of the network. Typically, a network has a set of filters per conv layer, and each produces a separate 2-dimensional activation map. The output volume of the layer is created by stacking these activation maps together along the depth dimension.

\subsection{Local connectivity}
When dealing with high-dimensional inputs such as images, it is impractical to connect all neurons to all neurons on the previous volume. Instead, neurons are only connected to a local region of the input volume. The spatial extend of this connectivity is a hyperparameter\footnote{The term \textit{hyperparameter} is often used to distinguish between the \textit{parameters} of a layer (i.e., its weights and bias) and the characteristics of a layer. The latter are referred to as hyperparameters.} called \textit{receptive field} of the neuron (which kind of expresses the size of the filter related to the neuron). The extend of the connectivity along the depth axis is always equal to the depth of the input volume. That is, every filter always considers all depth slices of the input volume, but not all the width and height (at once).

\begin{figure}[h!]
\begin{center}
\begin{tikzpicture}[font=\scriptsize,scale=0.65]
\draw[fill=hured!20,xyp=5] (0,0) rectangle (3,5);
\draw[fill=hured!40,yzp=3] (0,0) rectangle (5,5);
\draw[fill=hured!40,xzp=5] (0,0) rectangle (3,5);
\draw (1,0,5) -- (1,5,5) -- (1,5,0)
      (2,0,5) -- (2,5,5) -- (2,5,0);
\node at (0.5,0.25,5) {$i_1$};
\node at (1.5,0.25,5) {$i_2$};
\node at (2.5,0.25,5) {$i_3$};

\draw[fill=hublue!20,xyp=4,dashed] (7,0) rectangle (9, 3);
\draw[fill=hublue!40,yzp=9,dashed] (0,1) rectangle (3,4);
\draw[fill=hublue!40,xzp=3,dashed] (7,1) rectangle (9,4);
\draw[dashed] (8,0,4) -- (8,3,4) -- (8,3,1);
\node at (7.5,0.25,4) {$w_1$};
\node at (8.5,0.25,4) {$w_2$};

\draw[fill=green!20,xyp=4] (12,0) rectangle (14,3);
\draw[fill=green!40,yzp=14] (0,1) rectangle (3,4);
\draw[fill=green!40,xzp=3] (12,1) rectangle (14,4);
\draw (13,0,4) -- (13,3,4) -- (13,3,1);
\node at (12.5,0.25,4) {$o_1$};
\node at (13.5,0.25,4) {$o_2$};
\draw[->,ultra thick] (4.5,1.5,2.5) to (5.5,1.5,2.5);
\draw[->,ultra thick] (10, 1.5,2.5) to (11,1.5,2.5);
\end{tikzpicture}~\\[1cm]
\newcommand\inputVol[3]{
\def\numbers{#3}
\foreach \x in {0,...,6} \foreach \y in {0,...,6}
  {
    %\pgfmathparse{\x == 0 || \y == 0 || \x == 6 || \y == 6 ? "gray!40" : "hured!20"}
    \pgfmathparse{\x == 0 || \y == 0 || \x == 6 || \y == 6 ? 40 : 20}
    \edef\colour{\pgfmathresult}
    \path[fill=hured!\colour] (#1+\x, #2+\y) rectangle ++ (0.9,0.9);
    \pgfmathsetmacro\n{\numbers[\x+\y*7]}
    \node at (#1+\x+0.5, #2+\y+0.5) {\n};
  }
}
\newcommand\volume[4]{
\def\numbers{#4}
\foreach \x in {0,1,2} \foreach \y in {0,1,2} {
    \path[fill=#3!20] (#1+\x, #2+\y) rectangle ++ (0.9,0.9);
    \pgfmathsetmacro\n{\numbers[\x+\y*3]}
    \node at (#1+\x+0.5, #2+\y+0.5) {\pgfmathprintnumber{\n}};
  }
}
\begin{tikzpicture}[font=\scriptsize,scale=0.5]
\node[anchor=south west] at (0,22.5) {Input Volume (+pad 1) $(7\times 7\times 3)$};
\node[anchor=south west] at (0,22) {$i_1$:};
\inputVol{0}{15}{{
  0,0,0,0,0,0,0,
  0,0,0,1,1,2,0,
  0,0,1,2,0,0,0,
  0,1,1,2,0,1,0,
  0,2,1,1,0,0,0,
  0,0,2,1,2,1,0,
  0,0,0,0,0,0,0
}}
\node[anchor=south west] at (0,14.25) {$i_2$:};
\inputVol{0}{7.5}{{
  0,0,0,0,0,0,0,
  0,1,1,1,2,2,0,
  0,1,2,1,0,2,0,
  0,2,0,2,1,0,0,
  0,0,2,1,2,2,0,
  0,2,1,1,1,1,0,
  0,0,0,0,0,0,0
}}
\node[anchor=south west] at (0,6.75) {$i_3$:};
\inputVol{0}{0}{{
  0,0,0,0,0,0,0,
  0,2,0,0,2,0,0,
  0,1,2,2,2,1,0,
  0,2,0,1,2,0,0,
  0,2,2,1,1,2,0,
  0,2,1,2,1,2,0,
  0,0,0,0,0,0,0
}}
\foreach \x in {1,2,3} \foreach \y in {1,2,3} {
  \draw[hured,thick] (\x+1.95, 7.95-\y) rectangle (\x+0.95, 6.95-\y);
    \draw[hured,thick] (\x+1.95, 15.45-\y) rectangle (\x+0.95, 14.45-\y);
      \draw[hured,thick] (\x+1.95, 22.95-\y) rectangle (\x+0.95, 21.95-\y);
}
\volume{9}{18}{hublue}{{0,-1,0,1,-1,1,-1,-1,1}}
\volume{9}{14}{hublue}{{1,1,0,1,-1,0,0,-1,0}}
\volume{9}{10}{hublue}{{1,0,1,1,-1,1,-1,1,0}}
\foreach \x in {1,2,3} \foreach \y in {1,2,3} {
  \draw[hublue,thick] (\x+7.95, 13.95-\y) rectangle (\x+8.95, 12.95-\y);
    \draw[hublue,thick] (\x+7.95, 17.95-\y) rectangle (\x+8.95, 16.95-\y);
      \draw[hublue,thick] (\x+7.95, 21.95-\y) rectangle (\x+8.95, 20.95-\y);
}
\volume{14}{18}{hublue}{{1,-1,1,-1,0,-1,1,-1,0}}
\volume{14}{14}{hublue}{{0,0,1,-1,0,0,-1,0,-1}}
\volume{14}{10}{hublue}{{0,1,-1,1,1,1,0,1,1}}
\node[anchor=south west] at (9,21) {$w_1 (3\times 3\times 3)$:};
\node[anchor=south west] at (14,21) {$w_2 (3\times 3\times 3)$:};
\node[anchor=south west] at (9,8.5) {Bias $b_0 (1\times 1 \times 1)$};
\node[anchor=south west] at (14,8.5) {Bias $b_1 (1\times 1 \times 1)$};
\draw[hublue,thick,fill=hublue!20] (9,7.5) rectangle ++ (0.9,0.9);
\node[anchor=south west] at (9,7.5) {1};
\draw[fill=hublue!20,hublue!20] (14,7.5) rectangle ++ (0.9,0.9);
\node[anchor=south west] at (14,7.5) {0};

\volume{19}{18}{green}{{-1,-1,-1,1,0,6,0,9,6}}
\volume{19}{14}{green}{{3,1,0,3,-1,2,2,1,2}}
\draw[green,thick] (20, 20) rectangle ++(0.9, 0.9);

\node[anchor=south west] at (19,22) {Output volume $(3\times 3\times 2)$};
\node[anchor=south west] at (19,21) {$o_1$:};
\node[anchor=south west] at (19,17) {$o_2$:};

\draw[thick] (9,21) -- (5,22)
      (9,18) -- (2,19)
      (9,17) -- (2,14.5)
      (9,14) -- (5,11.5)
      (9,13) -- (2,7)
      (9,10) -- (5,4);
\end{tikzpicture}
\end{center}
\caption{Example of hyperparameters and calculations in convolutional layers. Using two filters of $3\times 3$ (with depth $3$, as the input has depth $3$),padding $1$, and stride $2$. Note that $w_1$ and $w_2$ are virtual: the neurons in the conv layer (in this case $o_1$ and $o_2$) are wired through the filters to the activation volume. That is, the neuron highlighted in output is actually wired (using the filters and weights specified in $w_1$) to the highlighted regions of the activation volume.}\label{fig:cnnCalc}
\end{figure}

\subsection{Spatial arrangement}
Neurons in the conv layer are thus locally connected to the input volume. Their arrangement in the output volume is determined by three hyperparameters: the \textit{depth}, \textit{stride}, and \textit{zero-padding}:
\begin{itemize}
\item First, \textit{depth} of the output volume corresponds to the number of filters that are used in the convolutional layer (each looking for something different in the input). For example, if the first convolutional layer takes as input the raw image, then different neurons along the depth dimension may activate in presence of various oriented edges, or blobs of colour. We refer to a set of neurons that are all looking at the same region of the input as a \textit{depth column} (or \textit{fibre}).
\item Second, the \textit{stride} determines the amount with which the filter slides across the input volume. When the stride is 1, the filter moves 1 pixel at a time. When the stride is 2 (or uncommonly 3 or more, though this is rare in practice) then the filter jumps 2 pixels at a time. This produces smaller output volumes spatially.
\item Sometimes it is convenient to pad the input volume with zeros around the border. The size of this \textit{zero-padding} is a hyperparameter. The nice feature of zero padding is that it allows us to control the spatial size of the output volumes (commonly, padding is used to preserve the spatial size of the input volume so the input and output width and height are the same)\footnote{To determine the amount of padding required to make the output volume spatially equal to the input volume, use the following formula (for stride 1): $P=(F-1)/2$.}.
\end{itemize}
The spatial size of the output volume can be computed as a function of the input volume size ($W$)\footnote{As width and height are typically the same, we only need to use either to compute the output size.}, the receptive field size of the conv layer neurons ($F$)\footnote{The receptive size of a conv layer is the size ($W_f\times H_f$) of the filters applied. All filters in a single layer typically have the same size, and are normally considered to be square (so $F$ is equal to either $W_f$ or $H_f$).}, the stride with which they are applied ($S$), and the amount of zero padding used ($P$) on the border. The correct formula for calculating how many neurons may ``fit'' is given by $(W-F+2P)/S+1$. For example, for a $7\times 7$ input and a $3\times 3$ filter with stride 1 and pad 0 we would get a $5\times 5$ output (there are only 5 unique ways to fit a width of 3 on a width of 7, with stride 1). With stride 2 we would get a $3\times 3$ output. Note that the answer to the function above needs to be an integer (a whole number), otherwise we cannot fit the filter in the input volume.

\subsection{Parameter sharing}
Parameter sharing scheme is used in convolutional layers to control the number of parameters (weights). As shown earlier, fully connecting all neurons and giving separate weights to each connection, quickly becomes unmanageable.

It turns out that the number of parameters can be drastically reduced by making a reasonable assumption: if one feature is useful to compute at some spatial position ($x$, $y$), then it should also be useful to compute at a different position ($x_2$, $y_2$). In other words, each of the neurons in a depth slice uses the same weights and bias.

Notice that if all neurons in a single depth slice are using the same weight vector, then the forward pass of the CONV layer can in each depth slice be computed as the \textbf{convolution} of the neuron's weights with the input volume (hence the name: convolution layer). This is also why it is common to refer to the set of weights as a \textit{filter} or \textit{kernel}, that is convolved with the input.

\subsection{Summary}
A convolutional layer:
\begin{itemize}
\item accepts a volume of size $W_1\times H_1\times D_1$;
\item requires four hyperparameters:
  \begin{itemize}
  \item number of filters $K$;
  \item their spatial extend $F$;
  \item the stride $S$;
  \item the amount of zero padding $P$.
  \end{itemize}
\item produces a volume of size $W_2\times H_2\times D_2$ where:
  \begin{itemize}
  \item $W_2 = (W_1-F+2P)/S+1$
  \item $H_2=(H_1-F+2P)/S+1$ (i.e. width and height are computed equally by symmetry)
  \item $D_2=K$
  \end{itemize}
\item with parameter sharing, it introduces $F\cdot F\cdot D_1$ weights per filter, for a total of $(F\cdot F\cdot D_1)\cdot K$ weights and $K$ biases;
\item in the output volume, the $d$-th depth slice (of size $W_2\times H_2$) is the result of performing a valid convolution of the $d$-th filter over the input volume with a stride of $S$, and then offset by $d$-th bias.
\end{itemize}
There are common conventions and rules of thumb that motivate these hyperparameters; for example, often $F=3$, $S=1$, and $P=1$ are used.

\subsection{Rectified linear unit (RELU) layer}
In chapter \ref{ch:nn} discussed as being a part of a neuron, in some architectures the activation function of the neurons is represented as a separate layer. This is nowadays more common, as it more  clearly represents the hyperparameters of the network, and is easier to perform mathematically\footnote{Instead of performing the activation function `within' the neuron, the neuron now only calculates the appropriate dot products, after which its output (the result of the dot products) is passed through a simple filtering function.}. 

\begin{figure}[h!]
\begin{center}
\scalebox{1.5}{
\begin{tikzpicture}
\draw (-0.1,0) -- (5.1,0);
\draw (2.5,0) -- (2.5,2.6);
\foreach \x in {0,...,20} {
  \draw (\x*0.25,0) -- (\x*0.25,0.05);
}
\foreach \y in {1,...,20} {
  \draw (2.5, \y*0.125) -- (2.55, \y*0.125);
}
\node at (0,-0.25) {\tiny $-10$};
\node at (1.25,-0.25) {\tiny $-5$};
\node at (3.75,-0.25) {\tiny $5$};
\node at (5,-0.25) {\tiny $10$};
\node at (2.25,0.5) {\tiny $2$};
\node at (2.25,1.0) {\tiny $4$};
\node at (2.25,1.5) {\tiny $6$};
\node at (2.25,2.0) {\tiny $8$};
\node at (2.25,2.5) {\tiny $10$};
\draw[hured!60,thick] (0,0) -- (2.5,0) -- (5,2.5);
\end{tikzpicture}}
\end{center}
\caption{Rectified Linear Unit (ReLU) activation function, which is zero when $x < 0$ and then linear with slope $1$ when $x > 0$.}\label{fig:relu}
\end{figure}

The Rectified Linear Unit as activation has become very popular in the last few years. It computes the function $f(x)=\mathit(max)(0,x)$. In other words, the activation is simply thresholded at zero (see image below). There are several pros and cons to using the ReLUs:
\begin{itemize}
\item [(+)] It was found to greatly accelerate the convergence of stochastic gradient descent compared to the sigmoid/tanh functions.
\item [(+)] Compared to tanh/sigmoid neurons that involve extensive operations, the ReLU can be implemented by simply thresholding a matrix of activations at zero.
\item [(-)] Unfortunately, ReLU units can be fragile during training and can ``die''. For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the gradient flowing trhough the unit will forever be zero from that point on. That is, the ReLU units can irreversibly die during training since they can get knocked off the data manifold. This may happen as much as 40\% of your network's neurons if the learning rate is set too high. With proper setting of the learning rate this is less frequently an issue.
\end{itemize}

\subsection{Pooling layer}
Pooling layers are commonly inserted periodically in-between successive conv layers in a convnet architecture. The function of a pooling layer is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to control overfitting. The pooling layer operates independently on every depth slice of the input and resizes it spatially, for instance using the MAX operation. The most common form used is a pooling layer with filters of size $2\times 2$ applied with a stride of 2. This downsamples every depth slice in the input by 2 along both the width and height, discarding 75$\%$ of the activations. Every MAX operation would in this case be taking a max over 4 numbers. The depth dimension remains unchanged. 
\begin{figure}[h!]
\begin{center}
\begin{tikzpicture}[font=\scriptsize,scale=0.85]
\node[anchor=south west,draw,fill=hured!20,minimum width=1cm, minimum height=1cm] at (0,4) {1};
\node[anchor=south west,draw,fill=hured!20,minimum width=1cm, minimum height=1cm] at (1,4) {1};
\node[anchor=south west,draw,fill=hured!20,minimum width=1cm, minimum height=1cm] at (0,3) {5};
\node[anchor=south west,draw,fill=hured!20,minimum width=1cm, minimum height=1cm] at (1,3) {6};
\node[anchor=south west,draw,fill=green!20,minimum width=1cm, minimum height=1cm] at (2,4) {2};
\node[anchor=south west,draw,fill=green!20,minimum width=1cm, minimum height=1cm] at (3,4) {4};
\node[anchor=south west,draw,fill=green!20,minimum width=1cm, minimum height=1cm] at (2,3) {7};
\node[anchor=south west,draw,fill=green!20,minimum width=1cm, minimum height=1cm] at (3,3) {8};
\node[anchor=south west,draw,fill=yellow!20,minimum width=1cm, minimum height=1cm] at (0,2) {3};
\node[anchor=south west,draw,fill=yellow!20,minimum width=1cm, minimum height=1cm] at (0,1) {1};
\node[anchor=south west,draw,fill=yellow!20,minimum width=1cm, minimum height=1cm] at (1,2) {2};
\node[anchor=south west,draw,fill=yellow!20,minimum width=1cm, minimum height=1cm] at (1,1) {2};
\node[anchor=south west,draw,fill=hublue!20,minimum width=1cm, minimum height=1cm] at (2,2) {1};
\node[anchor=south west,draw,fill=hublue!20,minimum width=1cm, minimum height=1cm] at (2,1) {3};
\node[anchor=south west,draw,fill=hublue!20,minimum width=1cm, minimum height=1cm] at (3,2) {0};
\node[anchor=south west,draw,fill=hublue!20,minimum width=1cm, minimum height=1cm] at (3,1) {4};

\node[anchor=south west,draw,fill=yellow!20,minimum width=1cm, minimum height=1cm] at (8,2) {3};
\node[anchor=south west,draw,fill=hured!20,minimum width=1cm, minimum height=1cm] at (8,3) {6};
\node[anchor=south west,draw,fill=hublue!20,minimum width=1cm, minimum height=1cm] at (9,2) {4};
\node[anchor=south west,draw,fill=green!20,minimum width=1cm, minimum height=1cm] at (9,3) {8};

\draw[thick,->] (-0.5,1) to (-0.5,5);
\draw[thick,->] (0, 0.5) to (4, 0.5);
\draw[thick,->] (4.5,3) to (7.5,3);
\node at (-0.75,4.5) {x};
\node at (3.5,0.25) {y};
\node[text width=2.5cm,anchor=west] at (4.5,3.5) {max pool with $2\times 2$ filters and stride 2};

\draw[fill=hured!20,xyp=2] (1,8) rectangle ++ (2,2);
\draw[fill=hured!30,yzp=3] (8,0) rectangle ++ (2,2);
\draw[fill=hured!20,xzp=10] (1,0) rectangle++ (2,2);
\draw[fill=hured!35,xyp=2] (1.5,8) rectangle ++ (0.15,2);
\draw[yzp=1.65,fill=hured!35] (8,0) rectangle++ (2,2);
\draw[fill=hured!35,xzp=10] (1.5,0) rectangle++ (0.15,2);
\draw (1,10,2) -- (3,10,2); 

\draw[fill=hured!20,xyp=1] (8,8) rectangle ++ (2,1);
\draw[fill=hured!30,yzp=10] (8,0) rectangle ++ (1,1);
\draw[fill=hured!20,xzp=9] (8,0) rectangle ++ (2,1);
\draw[fill=hured!35,xyp=1] (8.5,8) rectangle ++ (0.15,1);
\draw[fill=hured!35,yzp=8.65] (8,0) rectangle ++ (1,1);
\draw[fill=hured!35,xzp=9] (8.5,0) rectangle ++ (0.15,1);
\draw (8,9,1) -- (9,9,1);
\draw[->,thick] (4.5,9,2) to (7.5,9,2);

\draw[->,ultra thick] (1,7) to (1,5.5);
\draw[->,ultra thick] (8.5,5.5) to (8.5,7);
\end{tikzpicture}
\end{center}
\caption{Pooling layer downsamples the volume spatially, independently in each depth slice of the input volume}\label{fig:pool}
\end{figure}
More generally, the pooling layer:
\begin{itemize}
\item accepts a volume of size $W_1\times H_1\times D_1$;
\item requires two hyperparameters:
  \begin{itemize}
  \item their spatial extend $F$;
  \item the stride $S$.
  \end{itemize}
\item produces a volume of size $W_2\times H_2\times D_2$ where:
  \begin{itemize}
  \item $W_2=(W_1-F)/S+1$
  \item $H_2=(H1-F)/S+1$
  \item $D_2=D1$
  \end{itemize}
\item introduces zero parameters since it computes a fixed function of the input;
\item Note that it is not common to use zero-padding for Pooling layers (they do not add anything useful).
\end{itemize}
It is worth noting that there are only two commonly seen variations of the max pooling layer found in practice: a pooling layer with $F=3$, $S=2$ (also called overlapping pooling), and more commonly $F=2$,$S=2$. Pooling sizes with larger receptive fields are too destructive.

In addition to max pooling, the pooling units can also perform other functions, such as \textit{average pooling} or even \textit{L2-norm pooling}. Average pooling was often used historically but has recently fallen out of favour compared to the max pooling, which has been shown to work better in practice.

\subsection{Fully-connected layer}
Neurons in a fully-connected layer have full connections to all activations in the previous layer, as seen in regular neural networks. Their activations can hence be computed with a matrix multiplication followed by a bias offset. See Chapter \ref{ch:opencl} for more information.

\section{Architectures}
Convolutional networks are commonly made up of only three types of layers: CONV, POOL (we assume max pool unless stated otherwise) and FC. We also explicitly write the RELU activation function as a layer, which applies elementwise non-linearity.

\subsection{Layer patterns}
The most common form of a convnet architecture stacks a few CONV-RELU layers, follows them with POOL layers, and repeats this pattern until the image has been merged spatially to a small size. At some point, it is common to transistion to fully-connected layers. The last fully-connected layer holds the output, such as the class scores. In other words, the most common convnet architecture follows the pattern:
$$\mathtt{
INPUT \rightarrow [[CONV \rightarrow RELU]*N \rightarrow POOL?]*M \rightarrow [FC \rightarrow RELU]*K \rightarrow FC}$$
where the $\mathtt{*}$ denotes repetition, and the $\mathtt{POOL?}$ indicates an optional pooling layer. Moreover, $\mathtt{N \geq 0}$ (and usually $\mathtt{N \leq 3}$), $\mathtt{M \geq 0}$, $\mathtt{K \geq 0}$ (and usually $\mathtt{K < 3}$). 

It should be noted that it is preferred to use a small stack of filter CONV layers to one large receptive field CONV layer. For example, it can be shown that three $3\times 3$ CONV layers on top of each other (with respective RELUs, of course) perform as well as a single layer with a $7\times 7$ filter, while nearly halving the number of parameters required. Intuitively, stacking CONV layers with tiny filters as opposed to having one CONV layer with big filters allows us to express more powerfull features of the input, and with fewer parameters. As a practical disadvantage, though, multiple CONV layers require more memory to hold all the intermediate results when doing backpropagation.

\subsection{Layer sizing heuristics}
Until now we have omitted mentions of common hyperparamters used in each of the layers in a convnet.

The \textit{input layer} (that contains the image) should be divisible by 2 many times. Common numbers include 32, 64, 96, 224, 384, and 512.

The \textit{conv layer} should be using small filters (preferable $3\times 3$ or $5\times 5$, using a stride of $S=1$, and crucially, padding the input volume with zeros in such a way that the conv layer does not alter the spatial dimensions of the input. That is, when $F=3$, then use $P=1$ to retain the original size of the input. When $F=5$, $P=2$. Only use larger filter sizes (like $7\times 7$ or so) on the very first conv layer that is looking at the input image (remember, calculate the required padding with $P=(F-1)/2$).

The \textit{pool layers} are in charge of downsampling the spatial dimensions of the input. The most common setting is to use max pooling with $2\times 2$ receptive fields, and a stride of 2. Note that this discards exactly $75\%$ if the activations in the input volume. Another slightly less common setting is to use $3\times 3$ receptive fields with a stride of 2. It is very uncommon to see receptive field sizes for max pooling that are larger than 3 because the pooling is then too lossy and aggressive. This usually leads to worse performance.
